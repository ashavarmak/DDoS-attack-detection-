{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbxNnJQdQFBm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvRqejDKE8Zg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3123137-f61c-40f9-9e08-d47778948d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iedMRcjcFCP8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/Drive/MyDrive/475/train.csv')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Using .fit_transform function to fit label\n",
        "# encoder and return encoded label\n",
        "label = le.fit_transform(df[' Source IP'])\n",
        "df.drop(\" Source IP\", axis=1, inplace=True)\n",
        "\n",
        "# Appending the array to our dataFrame\n",
        "# with column name 'Purchased'\n",
        "df[\" Source IP\"] = label\n",
        " le = LabelEncoder()\n",
        "\n",
        "# Using .fit_transform function to fit label\n",
        "# encoder and return encoded label\n",
        "label = le.fit_transform(df[' Timestamp'])\n",
        "df.drop(\" Timestamp\", axis=1, inplace=True)\n",
        "\n",
        "# Appending the array to our dataFrame\n",
        "# with column name 'Purchased'\n",
        "df[\" Timestamp\"] = label\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Using .fit_transform function to fit label\n",
        "# encoder and return encoded label\n",
        "label = le.fit_transform(df[' Destination IP'])\n",
        "df.drop(\" Destination IP\", axis=1, inplace=True)\n",
        "\n",
        "# Appending the array to our dataFrame\n",
        "# with column name 'Purchased'\n",
        "df[\" Destination IP\"] = label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clustering\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Generate sample data\n",
        "\n",
        "\n",
        "X = df\n",
        "X, labels_true = make_blobs(\n",
        "    n_samples=750, cluster_std=0.4, random_state=0\n",
        ")\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Compute DBSCAN\n",
        "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
        "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.labels_\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "home=metrics.homogeneity_score(labels_true, labels)\n",
        "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
        "print(\"Homogeneity: %0.3f\" % home)\n",
        "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
        "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
        "print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels_true, labels))\n",
        "print(\n",
        "    \"Adjusted Mutual Information: %0.3f\"\n",
        "    % metrics.adjusted_mutual_info_score(labels_true, labels)\n",
        ")\n",
        "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels))\n",
        "\n",
        "# #############################################################################\n",
        "# Plot result\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Black removed and is used for noise instead.\n",
        "unique_labels = set(labels)\n",
        "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        # Black used for noise.\n",
        "        col = [0, 0, 0, 1]\n",
        "\n",
        "    class_member_mask = labels == k\n",
        "\n",
        "    xy = X[class_member_mask & core_samples_mask]\n",
        "    plt.plot(\n",
        "        xy[:, 0],\n",
        "        xy[:, 1],\n",
        "        \"o\",\n",
        "        markerfacecolor=tuple(col),\n",
        "        markeredgecolor=\"k\",\n",
        "        markersize=14,\n",
        "    )\n",
        "\n",
        "    xy = X[class_member_mask & ~core_samples_mask]\n",
        "    plt.plot(\n",
        "        xy[:, 0],\n",
        "        xy[:, 1],\n",
        "        \"o\",\n",
        "        markerfacecolor=tuple(col),\n",
        "        markeredgecolor=\"k\",\n",
        "        markersize=6,\n",
        "    )\n",
        "\n",
        "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yECX77nQScbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ARIMA\n",
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from matplotlib import pyplot\n",
        "#from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "# load dataset\n",
        "#def parser(x):\n",
        "#\treturn datetime.strptime('190'+x, '%Y-%m')\n",
        "#series = read_csv('shampoo-sales.csv')\n",
        "#series.index = series.index.to_period('M')\n",
        " #split into train and test sets\n",
        "X = series.values\n",
        "size = int(len(X) * 0.66)\n",
        "train, test = X[0:size], X[size:len(X)]\n",
        "history = [x for x in train]\n",
        "predictions = list()\n",
        "# walk-forward validation\n",
        "for t in range(len(test)):\n",
        "\tmodel = ARIMA(history, order=(1,1,0))\n",
        "\tmodel_fit = model.fit()\n",
        "\toutput = model_fit.forecast()\n",
        "\tyhat = output[0]\n",
        "\tpredictions.append(yhat)\n",
        "\tobs = test[t]\n",
        "\thistory.append(obs)\n",
        "\tprint('predicted=%f, expected=%f' % (yhat, obs))\n",
        "# evaluate forecasts\n",
        "rmse = sqrt(mean_squared_error(test, predictions))\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "# plot forecasts against actual outcomes\n",
        "pyplot.plot(test)\n",
        "pyplot.plot(predictions, color='red')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "os6HbynyStfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lypunav\n",
        "from math import log1p\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "with open('/content/Drive/MyDrive/timeseries.txt', 'r') as f:\n",
        "    data = [float(i) for i in f.read().split()]\n",
        "\n",
        "N = len(data)\n",
        "eps = 0.001\n",
        "lyapunovs = [[] for i in range(N)]\n",
        "\n",
        "for i in range(N):\n",
        "    for j in range(i + 1, N):\n",
        "        if np.abs(data[i] - data[j]) < eps:\n",
        "            for k in range(min(N - i, N - j)):\n",
        "                lyapunovs[k].append(log1p(np.abs(data[i+k] - data[j+k])))\n",
        "\n",
        "with open('lyapunov.txt', 'w') as f:\n",
        "    for i in range(len(lyapunovs)):\n",
        "        if len(lyapunovs[i]):\n",
        "            string = str(np.round( sum(lyapunovs[i]) / len(lyapunovs[i])))\n",
        "            f.write(string + '\\n')\n",
        "t2 = time.perf_counter()\n",
        "t3=t2-t1\n",
        "print('time taken to run in sec:',t3)\n"
      ],
      "metadata": {
        "id": "KNmpYOWQSynk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import boxcox\n",
        "from nolds import lyap_e\n",
        "\n",
        "# Predict values\n",
        "predictions\n",
        "# Calculate prediction errors\n",
        "prediction_errors = np.abs(Z_test - predictions)\n",
        "\n",
        "# Calculate Lyapunov exponent\n",
        "lyapunov_exponent = lyap_e(prediction_errors, emb_dim=10, min_tsep=10, min_neighbors=20)\n",
        "\n",
        "# Determine anomaly scores\n",
        "anomaly_scores = np.where(lyapunov_exponent > threshold_lambda, 1, 0)\n",
        "\n",
        "# Plot the Lyapunov exponent values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(lyapunov_exponent, label=\"Lyapunov Exponent\")\n",
        "plt.axhline(y=threshold_lambda, color='r', linestyle='--', label=\"Threshold\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Lyapunov Exponent\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print or analyze the anomaly scores\n",
        "print(\"Anomaly Scores:\")\n",
        "print(anomaly_scores)\n"
      ],
      "metadata": {
        "id": "1L7m2v4uTA7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MF7PwIxmUSfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import medfilt\n",
        "\n",
        "# Load or generate your data as a Pandas DataFrame, replace 'your_data.csv' with your dataset\n",
        "data = pd.read_csv('/content/Drive/MyDrive/475/train.csv')\n",
        "\n",
        "# Define constants\n",
        "alpha1 = 0.1\n",
        "alpha2 = 0.8\n",
        "q = 3  # Adjust this multiplier as needed for thresholding\n",
        "\n",
        "# Function to calculate the absolute difference between two exponential filters\n",
        "def absolute_difference(x, alpha1, alpha2):\n",
        "    f1 = f2 = 0\n",
        "    adf = []\n",
        "\n",
        "    for val in x:\n",
        "        f1 = alpha1 * val + (1 - alpha1) * f1\n",
        "        f2 = alpha2 * val + (1 - alpha2) * f2\n",
        "        adf.append(abs(f1 - f2))\n",
        "\n",
        "    return adf\n",
        "\n",
        "# Function to calculate the rolling median\n",
        "def rolling_median(x, window_size):\n",
        "    return medfilt(x, kernel_size=window_size)\n",
        "\n",
        "# Function to calculate least distance and threshold for anomaly detection\n",
        "def anomaly_detection(x, ld_mean, ld_std, q):\n",
        "    ldt = abs(x - ld_mean)\n",
        "    threshold = ld_mean + q * ld_std\n",
        "    score2 = 0 if ldt <= threshold else 1\n",
        "    return score2\n",
        "\n",
        "# Process the NUDA feature\n",
        "NUDA_feature = data['Destination IP'].values\n",
        "Adf = absolute_difference(NUDA_feature, alpha1, alpha2)\n",
        "M = rolling_median(Adf, window_size=5)  # Adjust the window size as needed\n",
        "\n",
        "# Compute the mean and standard deviation of least distances\n",
        "ld = []\n",
        "for i in NUDA_feature:\n",
        "    ld.append(min(abs(i - x) for x in NUDA_feature if x != i))\n",
        "ld_mean = np.mean(ld)\n",
        "ld_std = np.std(ld)\n",
        "\n",
        "# Set Model 2 to False as described in the algorithm\n",
        "Model_2 = False\n",
        "\n",
        "# Loop through incoming traffic instances\n",
        "testdata= data[ Destination IP]\n",
        "for yt in testdata  # Replace 'incoming_traffic' with your incoming data\n",
        "    ldt = abs(yt - NUDA_feature)\n",
        "    score2 = anomaly_detection(ldt, ld_mean, ld_std, q)\n",
        "\n",
        "    # Check if it's abnormal and take appropriate action\n",
        "    if score2 == 1:\n",
        "        # Implement your action for handling abnormal traffic here\n",
        "        print(\"Abnormal Traffic Detected\")\n",
        "    else:\n",
        "        print(\"Normal Traffic\")\n"
      ],
      "metadata": {
        "id": "6kmbiBvnVYJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables\n",
        "threshold = 0.6  # Adjust threshold as needed\n",
        "\n",
        "# Define a function for event correlation\n",
        "def event_correlation(cluster_scores):\n",
        "    # Calculate the sum of anomaly scores for all clusters using AND operation\n",
        "    final_scores = [score1 and score2 for score1, score2 in cluster_scores]\n",
        "\n",
        "    # Check if the sum of final anomaly scores is greater than the threshold\n",
        "    total_score = sum(final_scores)\n",
        "    if total_score > threshold:\n",
        "        return \"Abnormal Traffic\"\n",
        "    else:\n",
        "        return \"Normal Traffic\"\n",
        "\n",
        "# Main loop to process incoming traffic samples\n",
        "while True:\n",
        "    incoming_traffic = receive_traffic_sample()  # Receive a new traffic sample\n",
        "\n",
        "    # Process the traffic sample and obtain anomaly scores for each cluster\n",
        "    cluster_scores = []\n",
        "    for cluster in clusters:\n",
        "        score1, score2 = calculate_anomaly_scores(incoming_traffic, cluster)\n",
        "        cluster_scores.append((score1, score2))\n",
        "\n",
        "    # Perform event correlation to determine traffic type\n",
        "    traffic_type = event_correlation(cluster_scores)\n",
        "\n",
        "\n",
        "    if traffic_type == \"Abnormal Traffic\":\n",
        "\n",
        "        print(\"Abnormal Traffic Detected\")\n"
      ],
      "metadata": {
        "id": "wgfJy4P6a73W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}